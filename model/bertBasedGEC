import torch
from transformers import BertTokenizer, BertModel, BertForMaskedLM
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from util import process

err_path = "../data/err_1.txt"
cor_path = "../data/cor_1.txt"
err_text, cor_text = process.makeData(err_path, cor_path)

BATCH_SIZE = 2
dataset = process.MyDataSet(err_text, cor_text)
data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)

BertModel_DIR = "../../BertTest/bert-base-uncased"

class Encoder(nn.Module):
    def __init__(self, hiddenSize):
        super().__init__()
        self.bertModel = BertModel.from_pretrained(BertModel_DIR)
        for name, param in self.bertModel.named_parameters():
            param.requires_grad = False
        self.rnn = nn.GRU(input_size=hiddenSize, hidden_size=hiddenSize, bidirectional=True)
        self.fc = nn.Linear(hiddenSize*3, hiddenSize)
        self.norm = nn.LayerNorm(hiddenSize)

    def forward(self, input):
        embedding = self.bertModel(**input)['last_hidden_state'] # **input 指的是去除了字典的值
        # embedding = [batch * seq * hiddensize]
        bert_encoder_hidden = embedding.transpose(0,1)[0]
        embedding = embedding.transpose(0, 1)
        bert_encoder_hidden = bert_encoder_hidden.unsqueeze(0).expand(2,bert_encoder_hidden.shape[0], bert_encoder_hidden.shape[1])
        gru_encoder_output, gru_encoder_hidden = self.rnn(embedding, bert_encoder_hidden)
        # 加入BiGRU，在bert的词表示上建模，再与bert的cls做一个相加
        gru_encoder_hidden = torch.cat((gru_encoder_hidden[-1], gru_encoder_hidden[-2]),dim=1).unsqueeze(0)
        encoder_hidden_output = torch.cat((bert_encoder_hidden[0].unsqueeze(0), gru_encoder_hidden), dim=2)
        #加一个现形激活层
        encoder_hidden_output = F.tanh(self.fc(encoder_hidden_output))

        # output = self.x(output, dim=1)
        return encoder_hidden_output

class Decoder(nn.Module):
    def __init__(self, hiddenSize, dec_inputSize, vocabSize, max_len, dropout = 0.2):
        super(Decoder, self).__init__()
        self.outputSize = vocabSize
        self.dropout = dropout
        self.max_len = max_len
        self.bertModel = BertModel.from_pretrained(BertModel_DIR)
        for name, param in self.bertModel.named_parameters():
            param.requires_grad = False
        self.rnn = nn.GRU(input_size=dec_inputSize, hidden_size=hiddenSize,
                        dropout=dropout)
        self.fc = nn.Linear(dec_inputSize, vocabSize)
        self.norm = nn.LayerNorm(vocabSize)
        self.sf = nn.Softmax(dim=2)

    def forward(self, enc_output_hidden, dec_input, mode=0):
        if mode == 0:
            # enc_output_hidden = 1(seqlen)*batchsize*bert_hiddensize
            # dec_input = seq * batchsize * inputsize
            dec_input = self.bertModel(**dec_input)['last_hidden_state'].transpose(0,1)[:-1,:,:]
            # outputs = torch.zeros(dec_input.shape[0], dec_input.shape[1], self.outputSize)
            # for i in range(1, dec_input.shape[0]):
                # input = torch.cat((enc_output_hidden, dec_input[i-1].unsqueeze(0)), dim=2)
            # input = dec_input[i-1].unsqueeze(0)
            output,_ = self.rnn(dec_input, enc_output_hidden)
            output = self.sf((self.norm(self.fc(F.tanh(output)))))

            # outputs[i-1] = self.sf(fc_output)
            #     enc_output_hidden = output
            return output

        elif mode == 1:
            # enc_output_hidden = 1(seqlen)*batchsize*bert_hiddensize
            # dec_input = seq * batchsize * inputsize
            dec_input = self.bertModel(**dec_input)['last_hidden_state'].transpose(0,1)[:-1,:,:]
            # 接下来测试模式的解码策略采用集束搜索，生成单词时，遇到【seq】代表停止
            outputs = torch.zeros(self.max_len, 1 ,self.outputSize)
            end_flag = "[SEP]"
            true_len = 0
            pre_wordIdList = [bertTokenizer.convert_tokens_to_ids("[CLS]")]

            for i in range(self.max_len):
                output, hidden = self.rnn(dec_input, enc_output_hidden) # 1 * batchsize * 768

                pre_out = self.sf((self.norm(self.fc(F.tanh(output))))).squeeze(0) # seq * 30522
                pre_word = bertTokenizer.decode(torch.tensor(pre_out.argmax(1))) # 输出一个预测词
                if(pre_word == end_flag):
                    break;
                else:
                    outputs[i] = pre_out

                    pre_word = bertTokenizer.convert_tokens_to_ids(pre_word)
                    pre_wordIdList.append(pre_word)
                    type = [1] * len(pre_wordIdList)
                    bert_embedding_output = self.bertModel(torch.tensor(pre_wordIdList).unsqueeze(0), torch.tensor(type).unsqueeze(0))['last_hidden_state'].transpose(0,1)
                    dec_input = bert_embedding_output[-1:, : , :]
                    # dec_input = output
                    enc_output_hidden = hidden
                    true_len += 1

            return outputs[:true_len, :, :]




class Seq2seq(nn.Module):
    def __init__(self, encoder, decoder):
        super(Seq2seq, self).__init__()
        self.encoder = encoder
        self.decoder = decoder

    # mode represents train or test,0 for train, 1 for test
    def forward(self, enc_input, dec_input, mode=0):
        enc_dec_hidden = self.encoder(enc_input)
        output = self.decoder(enc_dec_hidden, dec_input, mode)
        return output


bertTokenizer = BertTokenizer.from_pretrained(BertModel_DIR)
vocabSize = len(bertTokenizer)

encoder = Encoder(768)
decoder = Decoder(768, 768, vocabSize, 128)
model = Seq2seq(encoder,decoder)
# input = bertTokenizer(text=s, return_tensors='pt', padding=True)
# # print(input)
# output = model(input)

# trg = F.one_hot(input['input_ids'], num_classes=vocabSize).float()
# print(trg)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

for epoch in range(500):
    model.train()
    iteration = 0
    epoch_loss = 0
    for err_text, cor_text in data_loader:
        input = bertTokenizer(text=err_text, return_tensors='pt', padding=True)
        trg = bertTokenizer(text=cor_text, return_tensors='pt', padding=True)
        output = model(input, trg)
        trg = F.one_hot(trg['input_ids'][:,1:], num_classes=vocabSize).float()

        loss = criterion(output.transpose(0,1), trg)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
        iteration = iteration+1
        output = torch.tensor(output.transpose(0,1).argmax(2, keepdim=True))
        for batch in range(output.shape[0]):
            print(bertTokenizer.decode(output.squeeze(2)[batch]))
        print('Epoch:', '%02d' % (epoch + 1), 'Iteration:', '%02d' % iteration, 'batch-loss = ',
              '{:.6f}'.format(epoch_loss / BATCH_SIZE))

def test(test_path, cor_path):
    # TODO:测试集上测试效果,beam search size= 1
    cor_text = []
    with open(test_path, encoding='utf-8') as f1:
        for line in f1.readlines():
            input = bertTokenizer(text=line.strip(), return_tensors='pt', padding=True)
            trg = bertTokenizer(text="", return_tensors='pt', padding=True)
            output = model(input, trg, 1)
            cor_text.append(bertTokenizer.decode(output.transpose(0,1).argmax(2, keepdim=True).squeeze(2)[0]))

    with open(cor_path, encoding='utf-8', mode='w') as f2:
        for line in cor_text:
            f2.write(line)
            f2.write('\n')
    f1.close()
    f2.close()

test("../data/test_1.txt", "../data/test_1_out.txt")

